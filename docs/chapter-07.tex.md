## 卷积神经网络

CNN中新出现了卷积层（Convolution层）和池化层（Pooling层）。

之前介绍的神经网络中，相邻层的所有神经元之间都有连接，这称为全
连接（fully-connected）。另外，我们用Affine层实现了全连接层。

<img src="./images/Affine-full-connected.png" style="zoom:50%;" />



<img src="./images/CNN-demo.png" style="zoom:50%;" />

需要注意的是，在图 7-2 的 CNN 中，靠近输出的层中使用了之前
的“Affi ne - ReLU”组合。此外，最后的输出层中使用了之前的“Affi ne -
Softmax”组合。

### 卷积层

#### 全连接层存在的问题

全连接层存在什么问题呢？那就是数据的形状被“忽视”了。

实际上，前面提到的使用
了 MNIST 数据集的例子中，输入图像就是 1 通道、高 28 像素、长 28 像素
的（1, 28, 28）形状，但却被排成1列，以784个数据的形式输入到最开始的
Affine层。

比如，空间上邻近的像素为相似的值、RBG的各个通道之间分别有密切的关联性、相距较远的像素之间没有什么关联等，3维形状中可能隐藏有值得提取的本质模式。但是，因为全连接层会忽视形状，将全部的输入数据作为相同的神经元
（同一维度的神经元）处理，所以无法利用与形状相关的信息。

卷积层的输入数据称为输入特征图（input feature map），输出数据称为输出特征图（output feature map）。

> 也就是卷积层能够保留输入的形状信息，空间信息。

#### 卷积运算

卷积层进行的处理就是卷积运算。卷积运算相当于图像处理中的“滤波
器运算。

<img src="./images/convolution-compute.png" style="zoom:50%;" />

在本例中，输入大小是
(4, 4)，滤波器大小是(3, 3)，输出大小是(2, 2)。

如图7-4所示，将各个位置上滤波器的元素和输入的对应元素相乘，然后再求和（有时将这个计算称为乘积
累加运算）。然后，将这个结果保存到输出的对应位置。

<img src="./images/convolution-compute-demo.png" style="zoom:50%;" />

加上偏置

<img src="./images/convolution-plus-b.png" style="zoom:50%;" />

#### 填充

在图7-6的例子中，对大小为(4, 4)的输入数据应用了幅度为1的填充。“幅
度为1的填充”是指用幅度为1像素的0填充周围。

通过填充，大小为(4, 4)的输入数据变成了(6, 6)的形状。
然后，应用大小为(3, 3)的滤波器，生成了大小为(4, 4)的输出数据。

<img src="./images/convolution-padding.png" style="zoom:50%;" />

> 使用填充主要是为了调整输出的大小。比如，对大小为(4, 4)的输入
> 数据应用(3, 3)的滤波器时，输出大小变为(2, 2)，相当于输出大小
> 比输入大小缩小了2个元素。这在反复进行多次卷积运算的深度网
> 络中会成为问题。因为如果每次进行卷积运算都会缩小
> 空间，那么在某个时刻输出大小就有可能变为1，导致无法再应用
> 卷积运算。为了避免出现这样的情况，就要使用填充。在刚才的例
> 子中，将填充的幅度设为1，那么相对于输入大小(4, 4)，输出大小
> 也保持为原来的(4, 4)。因此，卷积运算就可以在保持空间大小不变
> 的情况下将数据传给下一层。

#### 步幅

应用滤波器的位置间隔称为步幅（stride）。之前的例子中步幅都是1，如
果将步幅设为2，则如图7-7所示，应用滤波器的窗口的间隔变为2个元素。

<img src="./images/convolution-stride.png" style="zoom:50%;" />

综上，增大步幅后，输出大小会变小。而增大填充后，输出大小会变大。

这里，假设输入大小为(H, W)，滤波器大小为(FH, FW)，输出大小为
(OH, OW)，填充为P，步幅为S。此时，输出大小可通过式(7.1)进行计算。
$$
\begin{aligned}
O H &=\frac{H+2 P-F H}{S}+1 \\
O W &=\frac{W+2 P-F W}{S}+1
\end{aligned}\tag{7.1}
$$
${H+2 P-F H}$可以认为是剩下的可移动的总距离，除以S就能得出一共还能走几步。加上之前出发点的第一步。那么结果就是上面的式子了。

输出大小无法除尽时（结果是小数时），需要采取报错等对策。根据深度学习的框架的不同，当值无法除尽时，有时会向最接近的整数四舍五入，不进行报错而继续运行。

#### 3维数据的卷积运算

图7-8是卷积运算的例子，图7-9是计算顺序。这里以3通道的数据为例，
展示了卷积运算的结果。

<img src="./images/convolution-of-3D.png" style="zoom:50%;" />

通道方向上有多个特征图时，会按通道
进行输入数据和滤波器的卷积运算，并将结果相加，从而得到输出。

<img src="./images/convolution-of-3D-procedure.png" style="zoom:50%;" />

需要注意的是，在3维数据的卷积运算中，**输入数据和滤波器的通道数要设为相同的值**。在这个例子中，输入数据和滤波器的通道数一致，均为3。

#### 结合方块思考

把3维数据表示为多维数组
时，书写顺序为（channel, height, width）。

<img src="./images/convolution-with-square.png" style="zoom:50%;" />

多个过滤器的话

<img src="./images/convolution-with-multi-filters.png" style="zoom:50%;" />

作为4维数据，滤波器的权重数据要按(output_channel, input_
channel, height, width) 的顺序书写。比如，通道数为 3、大小为 5 × 5 的滤
波器有20个时，可以写成(20, 3, 5, 5)。

考虑偏置。

<img src="./images/convolution-with-b.png" style="zoom:50%;" />

#### 批处理

我们希望卷积运算也同样对应批处理。为此，需要将在各层间传递的数
据保存为4维数据。具体地讲，就是按(batch_num, channel, height, width)
的顺序保存数据。

<img src="./images/convolution-batch.png" style="zoom:50%;" />

### 池化层

池化是缩小高、长方向上的空间的运算。如图7-14所示，进行将
2 × 2的区域集约成1个元素的处理，缩小空间大小。

<img src="./images/convolution-pooling-layer.png" style="zoom:50%;" />

“Max
池化”是获取最大值的运算，“2 × 2”表示目标区域的大小。如图所示，从
2 × 2的区域中取出最大的元素。此外，这个例子将步幅设为了2，所以
2 × 2的窗口的移动间隔为2个元素。另外，一般来说，池化的窗口大小会
和步幅设定成相同的值。比如，3 × 3的窗口的步幅会设为3。

> 除了Max池化之外，还有Average池化，或许还有Min池化。

池化层的特征

- 没有要学习的参数
- 通道数不发生变化

- 对微小的位置变化具有鲁棒性（健壮）。

### 卷积层和池化层的实现

#### 4维数组

4维数据，比如数据的形状是(10, 1, 28, 28)，则它对应10个高为28、长为28、通道为1的数
据。

```python
>>> x = np.random.rand(10, 1, 28, 28) # 随机生成数据
>>> x.shape
(10, 1, 28, 28)
```

#### 基于im2col的展开

im2col是一个函数，将输入数据展开以适合滤波器（权重）。如图7-17所示，
对3维的输入数据应用 im2col后，数据转换为2维矩阵（正确地讲，是把包含
批数量的4维数据转换成了2维数据)。